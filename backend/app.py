"""FastAPI backend for AI Content Tool.\n\nThis service exposes a /generate endpoint that accepts a JSON payload:\n    {"content_type": "...", "topic": "...", "tone": "..."}\nand returns generated content from an OpenAI model.\n\nConfiguration:\n- Set OPENAI_API_KEY in environment or .env file.\n- Optionally set OPENAI_MODEL (default: gpt-4o-mini).\n- Optionally set OPENAI_BASE_URL if using a proxy/compatible endpoint.\n\nSecurity note: Never expose your API key to the browser. The backend holds the key.\n"""\n\nimport os\nfrom typing import Optional\n\nfrom fastapi import FastAPI, HTTPException\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom pydantic import BaseModel, Field\nfrom dotenv import load_dotenv\n\nfrom openai import OpenAI\n\n# Load environment variables from .env if present\nload_dotenv()\n\n# Read configuration\nOPENAI_API_KEY = os.getenv("OPENAI_API_KEY")\nOPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")\nOPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL")  # Optional override (e.g., Azure, proxy)\n\nif not OPENAI_API_KEY:\n    raise RuntimeError("OPENAI_API_KEY not set. Create a .env file or export env var.")\n\n# Initialize OpenAI client\nclient_kwargs = {"api_key": OPENAI_API_KEY}\nif OPENAI_BASE_URL:\n    client_kwargs["base_url"] = OPENAI_BASE_URL\nclient = OpenAI(**client_kwargs)\n\napp = FastAPI(title="AI Content Tool Backend")\n\n# Allow local dev origins (Streamlit default localhost ports)\norigins = [\n    "http://localhost",\n    "http://localhost:8501",  # Streamlit default\n    "http://127.0.0.1",\n    "http://127.0.0.1:8501",\n    "http://localhost:3000",  # Next.js typical\n]\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=origins,\n    allow_credentials=True,\n    allow_methods=["*"],\n    allow_headers=["*"],\n)\n\nclass ContentRequest(BaseModel):\n    content_type: str = Field(..., description="e.g., Blog Post, Social Media Caption, Ad Copy")\n    topic: str = Field(..., description="The subject, product, or idea to write about")\n    tone: str = Field("Casual", description="Tone of voice: Casual, Professional, Witty, etc.")\n    words: Optional[int] = Field(None, description="Optional target word count (approx)")\n\ndef build_prompt(content_type: str, topic: str, tone: str, words: Optional[int] = None) -> str:\n    """Return a natural-language instruction prompt for the model."""\n    base = f"You are an expert marketing copywriter. Write a {tone.lower()} {content_type.lower()} about {topic}."\n    if words:\n        base += f" Aim for roughly {words} words."\n    base += " Use clear, engaging language and include a call-to-action when appropriate."\n    return base\n\n@app.post("/generate")\nasync def generate_content(request: ContentRequest):\n    prompt = build_prompt(request.content_type, request.topic, request.tone, request.words)\n    try:\n        completion = client.chat.completions.create(\n            model=OPENAI_MODEL,\n            messages=[\n                {"role": "system", "content": "You are a helpful marketing and content generation assistant."},\n                {"role": "user", "content": prompt},\n            ],\n            temperature=0.8,\n            max_tokens=600,\n        )\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n    try:\n        content = completion.choices[0].message.content\n    except Exception:\n        raise HTTPException(status_code=500, detail="Unexpected response structure from model.")\n\n    return {"content": content, "model": OPENAI_MODEL, "prompt": prompt}\n\n@app.get("/health")\nasync def health() -> dict:\n    return {"status": "ok"}\n\nif __name__ == "__main__":\n    import uvicorn\n    uvicorn.run("app:app", host="0.0.0.0", port=8000, reload=True)\n